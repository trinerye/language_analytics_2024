{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that open and cleans the text files\n",
    "def open_and_clean_text(filenames):\n",
    "    # opens the files using a latin1 encoding which contains 191 characters from the Latin script, including finish letters    \n",
    "    with open(filenames, encoding='latin1') as file:\n",
    "        text = file.read()\n",
    "        # the re.sub function finds matching occurrences of a specified pattern and replaces it with an empty string\n",
    "        # r'\\<[^>]*\\>' is a regular expression which removes everything between the tags including empty tags\n",
    "        text_cleaned = re.sub(r'\\<[^>]*\\>', \"\", text) \n",
    "    return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which extracts the noun, verbs, adjectives, adverbs and the three unique entities person, loc, and org\n",
    "def extract_text_entities(doc):\n",
    "        \n",
    "        annotations = []\n",
    "        # iterates over each token in doc, appending the text and pos tags to the list annotations\n",
    "        # afterwards a dataframe is created of the list\n",
    "        for token in doc: \n",
    "            annotations.append([token.text, token.pos_])\n",
    "        # creates a pandas dataframe containing the text and the post\n",
    "        df = pd.DataFrame(annotations, columns=[\"text\", \"pos\"])\n",
    "\n",
    "        # filtrates the df using a boolean vector (the boolean vector is created from a conditional statement, \n",
    "        # checking if each element in pos is part of the list [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"])\n",
    "        df_keep = df[df['pos'].isin([\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"])] \n",
    "\n",
    "        # groups the elements in in pos and counts the size of each group\n",
    "        pos_count = df_keep.groupby(\"pos\").count()\n",
    "\n",
    "        # the same as before but this time, the \"-\" inverts the boolean vector removing the elements \n",
    "        # from pos which is found in the list [\"SPACE\", \"SYM\", \"PUNCT\", \"NUM\"]\n",
    "        df_removed = df[-df['pos'].isin([\"SPACE\", \"SYM\", \"PUNCT\", \"NUM\"])] \n",
    "\n",
    "        # calculates the normalization factor of df_removed divided by 10000\n",
    "        total_words = len(df_removed)/10000 \n",
    "       \n",
    "        # calculates the frequency of nouns, verbs, adjectives, and adverbs and rounds the number to a whole number \n",
    "        # cach POS frequency is normalized against the total word count for comparison\n",
    "        noun = round(pos_count[\"text\"][\"NOUN\"]/total_words)\n",
    "        verb = round(pos_count[\"text\"][\"VERB\"]/total_words)\n",
    "        adj = round(pos_count[\"text\"][\"ADJ\"]/total_words)\n",
    "        adv = round(pos_count[\"text\"][\"ADV\"]/total_words)\n",
    "\n",
    "        # creates a set for each unique entities (set can only hold unique values)\n",
    "        person, loc, org = set(), set(), set()\n",
    "        # iterates over each entity in doc, using a conditional statement to filter the entities, \n",
    "        # then adding each entity to the sets (person, loc, org) based on their labels\n",
    "        for entity in doc.ents:\n",
    "\n",
    "            if entity.label_ == \"PERSON\":\n",
    "                person.add(str(entity))\n",
    "\n",
    "            elif entity.label_ == \"LOC\":\n",
    "                loc.add(str(entity))\n",
    "            \n",
    "            elif entity.label_ == \"ORG\":\n",
    "                org.add(str(entity))\n",
    "       \n",
    "        # returns an int for each pos and unique entity \n",
    "        return noun, verb, adj, adv, len(person), len(loc), len(org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the main function with a parameter for the folder path (run the script here)\n",
    "def main(in_folderpath, out_folderpath):\n",
    "    # creates a sorted list of all the directories within the given folder path\n",
    "    dirs = sorted(os.listdir(in_folderpath))\n",
    "    # iterates over each directory in the sorted list 'dirs'\n",
    "    for directory in dirs:\n",
    "         # defines column names for the DataFrame.\n",
    "        columns = [\"filename\",\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PERSON\", \"LOC\", \"ORG\"]\n",
    "        # creating an empty dataframe for the final results\n",
    "        final_results = pd.DataFrame(columns=columns)\n",
    "        # creates a sorted list of all the filenames within each directory in the folder data\n",
    "        filenames = sorted(os.listdir(os.path.join(\"..\",\"in\", directory)))\n",
    "\n",
    "        # iterates over each text file in the sorted list 'filenames'.\n",
    "        for text_file in filenames:\n",
    "            # constructs the file path for each text file\n",
    "            filenames = os.path.join(\"..\",\"in\", directory, text_file)\n",
    "            # calls the open_and_clean_text(filenames) function \n",
    "            text_cleaned = open_and_clean_text(filenames)\n",
    "            # processes the cleaned text using the nlp model (en_core_web_md)\n",
    "            doc = nlp(text_cleaned)\n",
    "            # calls the extract_text_entities(doc) fucntion\n",
    "            noun, verb, adj, adv, person, loc, org = extract_text_entities(doc)\n",
    "            # creates a list containing the results \n",
    "            results = [text_file, noun, verb, adj, adv, person, loc, org]\n",
    "            # turns the results list into a dataframe using the predefined columns\n",
    "            df_results = pd.DataFrame([results], columns=columns)\n",
    "            # appends the results dataframe to the final results dataframe\n",
    "            final_results = pd.concat([final_results, df_results])\n",
    "\n",
    "        # saves the final results dataframe as a csv file in the out folder \n",
    "        final_results.to_csv(f\"{os.path.join(out_folderpath, directory)}.table.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     # calls the main function with two arguments \"../data\"  and \"../out\" defining the in- and out-folderpaths\n",
    "    main(\"../in\", \"../out\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
