{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 12 - Measuring environmental impact\n",
    "\n",
    "In this session, we're going to look at one particular way that we can measure the impact of our code on the world around us. In particular, we're going to be looking at how we can approximate the *environmental impact* of our cultural data science footprint.\n",
    "\n",
    "To do this, we're going to use the open-source software package *CodeCarbon*. You can find more information at the following links:\n",
    "\n",
    "- CodeCarbon Website: [https://codecarbon.io/](https://codecarbon.io/)\n",
    "- GitHub Repo: [https://mlco2.github.io/codecarbon/](https://mlco2.github.io/codecarbon/)\n",
    "- Documentation: [https://mlco2.github.io/codecarbon/](https://mlco2.github.io/codecarbon/)\n",
    "\n",
    "We'll do some testing on HuggingFace pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing HuggingFace pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:30:58.120367: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 10:30:58.124353: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 10:30:58.177361: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 10:30:59.150176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from codecarbon import EmissionsTracker\n",
    "from transformers import pipeline\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Text summarization pipeline__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may remember from a couple of weeks ago that *text summarization* was quite a compute intensive task. So let's see exactly how compute intensive it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. \n",
    "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. \n",
    "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. \n",
    "In the former task our best model outperforms even all previously reported ensembles.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54ff38c5fdb46e9900c3f597e7c4933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e538d6129940dd88d50b3456910767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a53973096c343ee8ea5e5f47dd993f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094fdc3eeddb4e3b92166dfa12d7f7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaeb6368309249f5bb08ab046623dc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline(task=\"summarization\", \n",
    "                      min_length=10,\n",
    "                      max_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different ways that we can work with CodeCarbon, all of which is clearly explained in the relevant documentation.\n",
    "\n",
    "We'll go through each of them one at a time here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Creating a tracker object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:31:51] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:31:51] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:31:51] No GPU found.\n",
      "[codecarbon INFO @ 10:31:51] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:31:51] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:31:52] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:31:52] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:31:52] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:31:52]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:31:52]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:31:52]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:31:52]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:31:52]   CPU count: 64\n",
      "[codecarbon INFO @ 10:31:52]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:31:52]   GPU count: None\n",
      "[codecarbon INFO @ 10:31:52]   GPU model: None\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714033916.942671   18723 service.cc:145] XLA service 0x56495f8d1e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1714033916.942811   18723 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "2024-04-25 10:31:56.950396: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1714033916.992247   18723 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[codecarbon INFO @ 10:32:04] Energy consumed for RAM : 0.000311 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:32:04] Energy consumed for all CPUs : 0.000094 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:32:04] 0.000405 kWh of electricity used since the beginning.\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.303957377374935e-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "summary = summarizer(text)\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - Context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:35:49] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:35:49] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:35:49] No GPU found.\n",
      "[codecarbon INFO @ 10:35:49] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:35:49] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:35:50] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:35:50] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:50] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:35:50]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:35:50]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:35:50]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:35:50]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:35:50]   CPU count: 64\n",
      "[codecarbon INFO @ 10:35:50]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:50]   GPU count: None\n",
      "[codecarbon INFO @ 10:35:50]   GPU model: None\n",
      "[codecarbon INFO @ 10:36:01] Energy consumed for RAM : 0.000319 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:36:01] Energy consumed for all CPUs : 0.000096 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:36:01] 0.000416 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'the Transformer replaces recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention .'}]\n"
     ]
    }
   ],
   "source": [
    "with EmissionsTracker() as tracker:\n",
    "    summary = summarizer(text)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 - A Python decoractor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecarbon import track_emissions\n",
    "\n",
    "@track_emissions\n",
    "def summarization(text):\n",
    "    summary = summarizer(text)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:41:41] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:41:41] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:41:41] No GPU found.\n",
      "[codecarbon INFO @ 10:41:41] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:41:41] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:41:42] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:41:42] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:41:42] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:41:42]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:41:42]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:41:42]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:41:42]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:41:42]   CPU count: 64\n",
      "[codecarbon INFO @ 10:41:42]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:41:42]   GPU count: None\n",
      "[codecarbon INFO @ 10:41:42]   GPU model: None\n",
      "[codecarbon INFO @ 10:41:53] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 10:41:53] Energy consumed for RAM : 0.000306 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:41:53] Energy consumed for all CPUs : 0.000092 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:41:53] 0.000398 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:41:53] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'the Transformer replaces recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention .'}]\n"
     ]
    }
   ],
   "source": [
    "summarization(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex example\n",
    "\n",
    "We can make the results more useful by changing the tracker parameters - full list can be found here [https://mlco2.github.io/codecarbon/parameters.html](https://mlco2.github.io/codecarbon/parameters.html).\n",
    "\n",
    "In the example that follows, we're going to download a HuggingFace dataset and a pretrained emotion classification model. \n",
    "\n",
    "We also introduce specific *tasks* to more clearly understand the impact of different parts of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:48:14] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:48:14] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:48:14] No GPU found.\n",
      "[codecarbon INFO @ 10:48:14] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:48:14] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:48:15] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:48:15] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:48:15] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:48:15]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:48:15]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:48:15]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:48:15]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:48:15]   CPU count: 64\n",
      "[codecarbon INFO @ 10:48:15]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:48:15]   GPU count: None\n",
      "[codecarbon INFO @ 10:48:15]   GPU model: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9eb84b5ea874f869c37764997b50e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2e37e858b54ab7a690e387dfee7047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eacbf8cd10c44808c9dadcf186bfaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545f9ad57df24ff188a4137b3834d13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fc68360748413083a2f8d1029955ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4ce828c40a4c36a0ca5f61d263f12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e26a551f7848f6b85ab04dc6bae9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:48:34] Energy consumed for RAM : 0.000625 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:48:34] Energy consumed for all CPUs : 0.000188 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:48:34] 0.000813 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4df67cb268349898093dd7ed553ddbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/768 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ffee314c72c482bbce6569bbcf7103e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c3e59012e749c0a3c2fabb9a568d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2de4fa197d44a19f3d126363bbdab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309109cd10d34677bd972255e5a0ca2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:48:43] Energy consumed for RAM : 0.000979 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:48:43] Energy consumed for all CPUs : 0.000295 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:48:43] 0.001273 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748ecf60b44d4c7aa7654bb8837e09d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 10:53:27] Background scheduler didn't run for a long period (284s), results might be inaccurate\n",
      "[codecarbon INFO @ 10:53:27] Energy consumed for RAM : 0.012126 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:53:27] Energy consumed for all CPUs : 0.003650 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:53:27] 0.015776 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:53:27] Energy consumed for RAM : 0.012126 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:53:27] Energy consumed for all CPUs : 0.003650 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:53:27] 0.015776 kWh of electricity used since the beginning.\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/codecarbon/output.py:197: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0028462549037700322"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfolder = os.path.join(\"..\", \"emissions\")\n",
    "os.makedirs(outfolder, exist_ok=True)\n",
    "\n",
    "tracker = EmissionsTracker(project_name=\"sentiment classification\",\n",
    "                           experiment_id=\"sentiment_classifier\",\n",
    "                           output_dir=outfolder,\n",
    "                           output_file=\"emissions_sentiment.csv\")\n",
    "\n",
    "# tracking data downloading\n",
    "tracker.start_task(\"load dataset\")\n",
    "dataset = datasets.load_dataset(\"imdb\", \n",
    "                                split=\"test\")\n",
    "imdb_emissions = tracker.stop_task()\n",
    "\n",
    "# tracking downloading and initializing model\n",
    "tracker.start_task(\"build model\")\n",
    "classifier = pipeline(task=\"sentiment-analysis\", \n",
    "                      model=\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "model_emissions = tracker.stop_task()\n",
    "\n",
    "# tracking classification pipeline\n",
    "tracker.start_task(\"run classification\")\n",
    "preds = []\n",
    "for row in tqdm(dataset[\"text\"][:1000]):\n",
    "    preds.append(classifier(row[:100]))\n",
    "classifier_emissions = tracker.stop_task()\n",
    "\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inspecting the results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_df = pd.read_csv(\"emissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000073\n",
       "1    0.000075\n",
       "2    0.000072\n",
       "Name: emissions, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emissions_df[\"emissions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "- Now that you have the basics down, head over and consider Assignment 5!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
